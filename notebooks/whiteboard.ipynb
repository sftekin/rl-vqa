{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1591, 9)\n",
      "(1591, 9)\n",
      "(1592, 9)\n",
      "(1592, 9)\n",
      "(1592, 9)\n",
      "(1592, 9)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "infer_dir = \"../results/inference\"\n",
    "\n",
    "# task_name = \"okvqa\"\n",
    "# ds_split = \"train\"\n",
    "\n",
    "# task_name = \"mmmu\"\n",
    "# ds_split = \"validation\"\n",
    "\n",
    "task_name = \"mmmu_pro\"\n",
    "ds_split = \"test\"\n",
    "\n",
    "\n",
    "model_names = [\"llava-v1.6-vicuna-7b-hf\", \"llava-v1.6-vicuna-13b-hf\",\n",
    "               \"Qwen2.5-VL-7B-Instruct\", \"InternVL2-8B\",\n",
    "               \"deepseek-vl2-tiny\", \"deepseek-vl2-small\"]\n",
    "\n",
    "\n",
    "def extract_letter(text):\n",
    "    match = re.search(r\"\\((\\w)\\)\", text)\n",
    "    return match.group(1) if match else \"\"\n",
    "\n",
    "error_list = []\n",
    "for mn in model_names:\n",
    "    data_path = os.path.join(infer_dir, task_name, ds_split, f\"{mn}_output.csv\")\n",
    "    data_df = pd.read_csv(data_path)\n",
    "\n",
    "    arr_path = os.path.join(infer_dir, task_name, ds_split, f\"{mn}_prob.npy\")\n",
    "    prob_arr = np.load(arr_path)\n",
    "\n",
    "    start_chr = 'A'\n",
    "    choices = []\n",
    "    for i in range(prob_arr.shape[1]):\n",
    "        choices.append(start_chr)\n",
    "        start_chr = chr(ord(start_chr) + 1)\n",
    "\n",
    "    prob_pred = []\n",
    "    for i in np.argmax(prob_arr, axis=1):\n",
    "        prob_pred.append(choices[i])\n",
    "    prob_pred = np.array(prob_pred, dtype=str)\n",
    "\n",
    "    generated_outputs = data_df[\"generated_outputs\"].values\n",
    "\n",
    "    extracted_outputs = []\n",
    "    for output in generated_outputs:\n",
    "        pred_txt = str(output)[:10].strip()\n",
    "        if \"\\n\" in pred_txt:\n",
    "            pred_txt = pred_txt.split(\"\\n\")[1]\n",
    "        if \"(\" in pred_txt or \")\" in pred_txt:\n",
    "            pred_txt = extract_letter(pred_txt)\n",
    "        extracted_outputs.append(pred_txt[:1].upper())\n",
    "    extracted_outputs = np.array(extracted_outputs)\n",
    "\n",
    "    labels = data_df[\"answer\"].values.astype(str) \n",
    "    if task_name == \"mmmu_pro\" and \"llava\" not in mn:\n",
    "        extracted_outputs = np.delete(extracted_outputs, (1017), axis=0)\n",
    "        prob_pred = np.delete(prob_pred, (1017), axis=0)\n",
    "        labels = np.delete(labels, (1017), axis=0)\n",
    "\n",
    "    errors = labels == extracted_outputs.astype(str)\n",
    "    error_list.append(errors.astype(int))\n",
    "    acc = np.mean(errors)\n",
    "\n",
    "    print(prob_arr.shape)\n",
    "    # print(mn, acc, np.mean(data_df[\"answer\"].values.astype(str) == prob_pred))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "exp_dict = torch.load(\"../results/ensemble/exp_result.pth\")\n",
    "error_arr = np.array(error_list)\n",
    "\n",
    "logits = exp_dict[\"logits\"]\n",
    "ens_preds = logits[:, -9:].argmax(axis=1)\n",
    "ens_err = (ens_preds == exp_dict[\"labels\"]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "split = \"test\"\n",
    "data_path = f\"../results/inference/ocr/{split}\"\n",
    "\n",
    "model_names = [\"llava-v1.6-vicuna-7b-hf\", \"llava-v1.6-vicuna-13b-hf\", \n",
    "               \"Qwen2.5-VL-7B-Instruct\", \"InternVL2-8B\", \n",
    "               \"deepseek-vl2-tiny\", \"deepseek-vl2-small\"]\n",
    "\n",
    "model_outputs = []\n",
    "answers = []\n",
    "questions = []\n",
    "for mn in model_names:\n",
    "    data_df = pd.read_csv(f\"{data_path}/{mn}_output.csv\", index_col=0)\n",
    "    model_outputs.append(data_df[\"generated_outputs\"].values)\n",
    "    if len(answers) == 0:\n",
    "        answers = data_df[\"answer\"].values\n",
    "        questions = data_df[\"question\"].values\n",
    "model_outputs = np.array(model_outputs).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3001, 6), (3001,), (3001,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs.shape, answers.shape, questions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.metrics.scores import f_measure\n",
    "\n",
    "\n",
    "def calc_metric(labels, pred_arr):\n",
    "    blue_scores, em_scores, F1 = [], [], []\n",
    "    for i in range(len(labels)):\n",
    "        pred = str(pred_arr[i]).strip().split(\" \")\n",
    "        blue_scores.append(nltk.translate.bleu_score.\n",
    "                           sentence_bleu([labels[i].split(\" \")], pred,\n",
    "                                         weights=(1, 0, 0, 0)))\n",
    "\n",
    "        reference_set = set(labels[i].split(\" \"))\n",
    "        test_set = set(pred)\n",
    "        F1.append(f_measure(reference_set, test_set))\n",
    "\n",
    "        pred = \" \".join(pred)\n",
    "        ans = re.sub(r\"[()]\", \"\", labels[i]).split(\" \")\n",
    "\n",
    "        txt_wo_prn = re.sub(r'\\([^)]*\\)', '', labels[i])\n",
    "        txt_with_prn = ans\n",
    "        if txt_wo_prn == pred or txt_with_prn == pred:\n",
    "            em_scores.append(1)\n",
    "        else:\n",
    "            em_scores.append(0)\n",
    "\n",
    "    return np.mean(blue_scores), np.mean(em_scores), np.mean(F1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/stekin6/.conda/envs/llamas/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/hice1/stekin6/.conda/envs/llamas/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/hice1/stekin6/.conda/envs/llamas/lib/python3.9/site-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for i, mn in enumerate(model_names):\n",
    "    blue_sc, em_sc, f1_sc = calc_metric(answers, model_outputs[:, i])\n",
    "    scores[mn] = {\"bleu_score\": blue_sc, \"em_score\": em_sc, \"f1_sc\": f1_sc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llava-v1.6-vicuna-7b-hf': {'bleu_score': 0.7807903437741843,\n",
       "  'em_score': 0.6771076307897368,\n",
       "  'f1_sc': 0.7960851736144892},\n",
       " 'llava-v1.6-vicuna-13b-hf': {'bleu_score': 0.7893527761555877,\n",
       "  'em_score': 0.6917694101966011,\n",
       "  'f1_sc': 0.8044668447086526},\n",
       " 'Qwen2.5-VL-7B-Instruct': {'bleu_score': 0.8539193058784806,\n",
       "  'em_score': 0.7517494168610463,\n",
       "  'f1_sc': 0.8679192066008536},\n",
       " 'InternVL2-8B': {'bleu_score': 0.5236647506317215,\n",
       "  'em_score': 0.4555148283905365,\n",
       "  'f1_sc': 0.5709381156177536},\n",
       " 'deepseek-vl2-tiny': {'bleu_score': 0.546012094714602,\n",
       "  'em_score': 0.4171942685771409,\n",
       "  'f1_sc': 0.5725242791702768},\n",
       " 'deepseek-vl2-small': {'bleu_score': 0.383092169394673,\n",
       "  'em_score': 0.29323558813728756,\n",
       "  'f1_sc': 0.4171195235151174}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llava-v1.6-vicuna-7b-hf\t78.079\t67.711\t79.609\n",
      "llava-v1.6-vicuna-13b-hf\t78.935\t69.177\t80.447\n",
      "Qwen2.5-VL-7B-Instruct\t85.392\t75.175\t86.792\n",
      "InternVL2-8B\t52.366\t45.551\t57.094\n",
      "deepseek-vl2-tiny\t54.601\t41.719\t57.252\n",
      "deepseek-vl2-small\t38.309\t29.324\t41.712\n"
     ]
    }
   ],
   "source": [
    "for key, values in scores.items():\n",
    "    print(f\"{key}\\t{values['bleu_score']*100:.3f}\\t{values['em_score']*100:.3f}\\t{values['f1_sc']*100:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, tokenized_inputs, labels, global_attention_tokens=None, negative_inputs=None):\n",
    "        self.tokenized_inputs = tokenized_inputs\n",
    "        self.labels = labels\n",
    "        self.global_attention_tokens = global_attention_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # input_ids = self.tokenized_inputs['input_ids'][idx]\n",
    "        # attention_mask = self.tokenized_inputs['attention_mask'][idx]\n",
    "        input_ids = torch.tensor(self.tokenized_inputs[idx].ids)\n",
    "        attention_mask = torch.tensor(self.tokenized_inputs[idx].attention_mask)\n",
    "        global_attentions = []\n",
    "        start = False\n",
    "        for i in input_ids:\n",
    "            if start:\n",
    "                if i == 50266:\n",
    "                    start = False\n",
    "                global_attentions.append(1)\n",
    "            else:\n",
    "                if i == 50265:\n",
    "                    start = True\n",
    "                global_attentions.append(0)\n",
    "        global_attentions = torch.tensor(global_attentions)        # token_type_ids = self.tokenized_inputs['token_type_ids'][idx]\n",
    "        label = self.labels[idx]\n",
    "        return_dict = {'input_ids': input_ids,\n",
    "                       \"labels\": label,\n",
    "                       'attention_mask': attention_mask,\n",
    "                       'global_attention_mask': global_attentions}\n",
    "\n",
    "        return return_dict\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
